{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the file with novels\n",
    "path = \"./Novels\"\n",
    "\n",
    "# Save all the titles of the texts\n",
    "textName = []\n",
    "\n",
    "# Save all the content of the texts\n",
    "textContent = []\n",
    "\n",
    "# Go to the directory with all the text files\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    # Add the file name and remove the file type (in this case \".txt\")\n",
    "    textName.append(filename[:-4])\n",
    "    \n",
    "    # Open each file and add all the content \n",
    "    with open(path + '/' + filename, \"r\") as file:\n",
    "         fileContent = file.read()\n",
    "\n",
    "    # Add the content of the file\n",
    "    textContent.append(fileContent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply stop words to all the text content\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Use snowballer to break words into their roots\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize and stem is used to break each token down into their base components. This is done to make it simplier\n",
    "# on the algorthm later\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    filtered_tokens = []\n",
    "\n",
    "    # Using regular expression break remove all token not containing letters\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    # Stem each word to their root word\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "# This is used to return stemmed words back to their original form. \n",
    "def tokenize_only(text):\n",
    "    \n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "            \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stemmed = []\n",
    "text_tokenized = []\n",
    "\n",
    "# iterate through all the text\n",
    "for work in textContent:\n",
    "    \n",
    "    # Store the text that has been stemmed into text stemmed\n",
    "    alltext_stemmed = tokenize_and_stem(work)\n",
    "    text_stemmed.extend(alltext_stemmed)\n",
    "    \n",
    "    # Store the all the text that has not been stemmed into text tokenized\n",
    "    alltext_tokenized = tokenize_only(work)\n",
    "    text_tokenized.extend(alltext_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a panda dataframe that has stemmed words as index and tokenized words as columns\n",
    "# This is so that words like \"happened, happening\" are all mapped to the same index \"happen\"\n",
    "vocab_frame = pd.DataFrame({'words': text_tokenized}, index = text_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 18s, sys: 172 ms, total: 2min 18s\n",
      "Wall time: 2min 18s\n",
      "(24, 28124)\n"
     ]
    }
   ],
   "source": [
    "# max_df: max cut off for how frequent a term appears in the collection\n",
    "# min_df: min start point to be considered a feature\n",
    "# max_features: maximum amount of features that can exist\n",
    "# Tokenizer: gave the previously defined tokenizer and stem program\n",
    "# ngram_range: Declare that I want to look at unigrams, bigrams and trigrams\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(textContent)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.89 s, sys: 84 ms, total: 7.98 s\n",
      "Wall time: 4.05 s\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books = { 'title': textName, 'content': textContent, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(books, index = [clusters] , columns = ['title', 'cluster', 'content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    8\n",
       "3    8\n",
       "2    3\n",
       "1    3\n",
       "0    2\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: catherine,\n",
      " s,\n",
      " elizabeth,\n",
      " jane,\n",
      " henry,\n",
      " charlotte,\n",
      "\n",
      "\n",
      "\n",
      "Cluster 0 titles: Northanger Abbey, Pride and Prejudice,\n",
      "\n",
      "\n",
      "Cluster 1 words: holmes,\n",
      " 's,\n",
      " watson,\n",
      " said,\n",
      " n't,\n",
      " sherlock,\n",
      "\n",
      "\n",
      "\n",
      "Cluster 1 titles: The Hound of the Baskervilles, The Adventures of Sherlock Holmes, The Sign of the Four,\n",
      "\n",
      "\n",
      "Cluster 2 words: holmes,\n",
      " s,\n",
      " t,\n",
      " watson,\n",
      " mr.,\n",
      " lestrade,\n",
      "\n",
      "\n",
      "\n",
      "Cluster 2 titles: The Valley of Fear, The Return of Sherlock Holmes, A Study In Scarlet,\n",
      "\n",
      "\n",
      "Cluster 3 words: 's,\n",
      " n't,\n",
      " anne,\n",
      " susan,\n",
      " olive,\n",
      " professor,\n",
      "\n",
      "\n",
      "\n",
      "Cluster 3 titles: Sense and Sensibility, Persuasion, Tales of Terror and Mystery, The Lost World, Bleak House, Oliver Twist, Lady Susan, A Christmas Carol,\n",
      "\n",
      "\n",
      "Cluster 4 words: s,\n",
      " t,\n",
      " don,\n",
      " don,\n",
      " fanny,\n",
      " emma,\n",
      "\n",
      "\n",
      "\n",
      "Cluster 4 titles: Emma, The Pickwick Papers, Our Mutual Friend, Mansfield Park, A Tale of Two Cities, Great Expectations, The Life And Adventure Of Nicholas Nickleby, David Copperfield,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "clusterTerm = []\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "\n",
    "    term = ''\n",
    "    for ind in order_centroids[i, :6]:\n",
    "        title = vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0]\n",
    "        print(\" \" + title + \",\")\n",
    "        term = term + title + \", \"\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "\n",
    "    try:\n",
    "        for title in frame.ix[i]['title'].values.tolist():\n",
    "            print(' %s,' % title, end='')\n",
    "    except:\n",
    "        print (' %s' % frame.ix[i]['title'], end = '')\n",
    "        \n",
    "    clusterTerm.append(term)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['catherine, s, elizabeth, jane, henry, charlotte, ',\n",
       " \"holmes, 's, watson, said, n't, sherlock, \",\n",
       " 'holmes, s, t, watson, mr., lestrade, ',\n",
       " \"'s, n't, anne, susan, olive, professor, \",\n",
       " 's, t, don, don, fanny, emma, ']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MDS()\n",
    "\n",
    "# two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Strip away Proper Nouns\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a'}\n",
    "\n",
    "cluster_names = {}\n",
    "keys = range(len(clusterTerm))\n",
    "\n",
    "\n",
    "\n",
    "values = [\"Hi\", \"I\", \"am\", \"John\"]\n",
    "for i in keys:\n",
    "        dicts[i] = values[i]\n",
    "print(dicts)\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Family, home, war', \n",
    "                 1: 'Police, killed, murders', \n",
    "                 2: 'Father, New York, brothers', \n",
    "                 3: 'Dance, singing, love', \n",
    "                 4: 'Killed, soldiers, captain'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
